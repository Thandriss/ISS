{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7916992,"sourceType":"datasetVersion","datasetId":4651984}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport datetime\nimport os\nimport time\nimport matplotlib.pyplot as plt\nimport gc\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The corpus is formatted as a CSV and contains the following fields:\n\n* id\n* domain\n* type\n* url\n* content\n* scraped_at\n* inserted_at\n* updated_at\n* title\n* authors\n* keywords\n* meta_keywords\n* meta_description\n* tags\n* summary\n* source (opensources, nytimes, or webhose)","metadata":{}},{"cell_type":"markdown","source":"Examples\n0,141,awm.com,unreliable,http://awm.com/church-congregation-brings-gift-to-waitresses-working-on-christmas-eve-has-them-crying-video/,\"Sometimes the power of Christmas.....the year.\",2018-01-25 16:17:44.789555,2018-02-02 01:19:41.756632,2018-02-02 01:19:41.756664,\"Church Congregation Brings Gift to Waitresses Working on Christmas Eve, Has Them Crying (video)\",Ruth Harris,,[''],,,","metadata":{}},{"cell_type":"markdown","source":"# **Splitting Data**\nBecause the the main data file is 6.45 GB. We needs to splits the data to smaller chunks. This reduces the processing time for our data pipelines and prevents out of memory errors. First we will select only the neccessary variables from the main file. The following variables are ignored based on these criterias\n* **Necessary**\n    * scraped_at\n    * inserted_at\n    * updated_at\n\n* **Unbalanced data (Too many empty values**\n    * authors\n    * keywords\n    * meta_keywords\n    * meta_description\n    * tags\n    * summary\n    * source (opensources, nytimes, or webhose)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:18:42.406130Z","iopub.execute_input":"2024-03-22T17:18:42.406677Z","iopub.status.idle":"2024-03-22T17:18:43.798792Z","shell.execute_reply.started":"2024-03-22T17:18:42.406644Z","shell.execute_reply":"2024-03-22T17:18:43.797527Z"}}},{"cell_type":"code","source":"# Defined chosen columns\ncolumns = ['id', 'domain', 'type', 'content', 'title']\ndtypes = {\n    'id':'object',\n    'domain':'object',\n    'type':'object',\n    'content':'object',\n    'title': 'object',\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf = pd.read_csv('/kaggle/input/fake-news-corpus-smaller/news_cleaned_2018_02_13.csv', usecols=columns, dtype=dtypes, engine='python', on_bad_lines='skip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preview data\nprint(df.info())\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Cleaning null data**\nNow we need to check whether there are empty values","metadata":{}},{"cell_type":"code","source":"#Detect Null and NaN values\ndf1 = df[df.isna().any(axis=1)]\ndf1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are `NaN` values at type columns, making these columns unusable. Therefore, we have to delete it","metadata":{}},{"cell_type":"code","source":"# Drop NaN values\ndf1 =  df.dropna() \ndf1.reset_index(drop = True, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's preview it again.","metadata":{}},{"cell_type":"code","source":"print(df1.info())\ndf1.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = '/kaggle/working/'\n#Import dropNAN data\ndf1.to_csv(f'{output_dir}news_noNull_.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Split files to smaller chunks**\n1. Now that the orginal looks cleaned. We needed to divide the dataset into smaller chunks otherwise it will takes days to finished preprocessing our data. We will split it into 250,000 rows per chunk.\n","metadata":{}},{"cell_type":"code","source":"# Define the path to the main CSV file\nmain_csv_path = f'{output_dir}news_noNull_.csv'\n\n# Define the number of rows per chunk\nrows_per_chunk = 250000  # Adjust the number of rows per chunk as needed\n\n# Read the headers separately\nwith open(main_csv_path, 'r') as f:\n    header = f.readline().strip().split(',')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because each chunks will be splitted by rows, only the first chunk will have headers. So we have to manually add it.","metadata":{}},{"cell_type":"code","source":"header","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below codes will interate throgh the main csv until there are no more rows. The `Error: EmptyDataError occurred while reading the file.` can be ignored","metadata":{}},{"cell_type":"code","source":"# Initialize the starting index for reading chunks\nstart_index = 1  # Start after the header\n\n# Initialize the counter for naming the output files\nfile_counter = 1\n\n# Read the main CSV file in chunks until there are no more rows\nwhile True:\n    try:\n        # Read the next chunk of rows\n        chunk = pd.read_csv(main_csv_path, skiprows=start_index, nrows=rows_per_chunk, header=None)\n\n        # Check if the chunk is empty\n        if chunk.empty:\n            print(\"Warning: Empty chunk encountered. Skipping...\")\n            break\n    \n        # Define the path to the output CSV file\n        output_csv_path = f'news_file_{file_counter}.csv'\n\n        # Write the chunk to the output CSV file with headers\n        chunk.to_csv(output_csv_path, index=False, header=header)\n\n        # Increment the file counter\n        file_counter += 1\n\n        # Update the starting index for the next chunk\n        start_index += rows_per_chunk\n    except pd.errors.EmptyDataError:\n        print(\"Error: EmptyDataError occurred while reading the file.\")\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Preview the chunk files**","metadata":{}},{"cell_type":"markdown","source":"Here we use datatable to quickly read the tables without exhausting the memory","metadata":{}},{"cell_type":"code","source":"!pip install datatable","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datatable as dt\n\n%%time\nraw_1 = dt.fread(\"/kaggle/working/news_file_1.csv\")\nraw_2 = dt.fread(\"/kaggle/working/news_file_2.csv\")\nraw_3 = dt.fread(\"/kaggle/working/news_file_3.csv\")\nraw_4 = dt.fread(\"/kaggle/working/news_file_4.csv\")\nraw_5 = dt.fread(\"/kaggle/working/news_file_5.csv\")\nraw_6 = dt.fread(\"/kaggle/working/news_file_6.csv\")\nraw_7 = dt.fread(\"/kaggle/working/news_file_7.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(raw_1.shape)\nprint(raw_2.shape)\nprint(raw_3.shape)\nprint(raw_4.shape)\nprint(raw_5.shape)\nprint(raw_6.shape)\nprint(raw_7.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Text Preprocessing**\nThe following steped were used to preprocess raw data\n\n1. Removing HTML tags\n2. Removing accented characters\n3. Expanding Contractions\n4. Removing Special Characters\n5. Lemmatization\n6. Removing Stopwords\n\nThe below code will be used to incrementally preprocess all data files","metadata":{}},{"cell_type":"code","source":"%%time\n# ADD PATH TO DATA FILE HERE\nfile_path = \"/kaggle/input/fake-news-chunks/news_file_1.csv\" # Edit for each increments\n\nrawdf = pd.read_csv(file_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preview dataset\nrawdf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use a sample text to test each function","metadata":{}},{"cell_type":"code","source":"sample_text = \"The Los Angeles Police Department has been denied $3 million in federal aid for law enforcement. While there is no official announcement as to why, it is more than likely that it has everything to do with LA’s “sanctuary city” status for harboring illegal aliens. Donald Trump and Attorney General Jeff Sessions have repeatedly said\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **spaCy**\n\n[`spaCy`](https://spacy.io/) is a free, open-source Python library that provides advanced capabilities to conduct natural language processing (NLP) on large volumes of text at high speed. These models are the power engines of spaCy. These models enable spaCy to perform several NLP related tasks, such as part-of-speech tagging, named entity recognition, and dependency parsing.\n\nI’ve listed below the different statistical models in spaCy along with their specifications:\n\n* en_core_web_sm: English multi-task CNN trained on OntoNotes. Size – 11 MB\n* en_core_web_md: English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Size – 91 MB\n* en_core_web_lg: English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Size – 789 MB\n\nFor this notebook, we used `en_core_web_md`","metadata":{}},{"cell_type":"code","source":"# Only run if SpaCy package is not installed\n# !pip install -U spacy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m spacy download en_core_web_md","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport requests\nimport re\nfrom bs4 import BeautifulSoup\n\nimport unicodedata\nnlp = spacy.load('en_core_web_md')\nimport en_core_web_md\nnlp = en_core_web_md.load()\ntokenizer = ToktokTokenizer()\nstopword_list = nltk.corpus.stopwords.words('english')\nstopword_list.remove('no')\nstopword_list.remove('not')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1. Removing HTML tags**\n\nOften, unstructured text contains a lot of noise, especially if you use techniques like web or screen scraping. HTML tags are typically one of these components which don’t add much value towards understanding and analyzing text.","metadata":{}},{"cell_type":"code","source":"# Remove HTML Tags\ndef strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    stripped_text = soup.get_text()\n    return stripped_text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the function\nstrip_html_tags(sample_text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2. Removing accented characters**\n\nUsually in any text corpus, you might be dealing with accented characters/letters, especially if you only want to analyze the English language. Hence, we need to make sure that these characters are converted and standardized into ASCII characters. A simple example — converting é to e.","metadata":{}},{"cell_type":"code","source":"def remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **3. Expanding Contractions**\n\nContractions are shortened version of words or syllables. They often exist in either written or spoken forms in the English language. These shortened versions or contractions of words are created by removing specific letters and sounds. In case of English contractions, they are often created by removing one of the vowels from the word. Examples would be, do not to don’t and I would to I’d. Converting each contraction to its expanded, original form helps with text standardization.\nWe leverage a standard set of `CONTRACTION_MAP` below","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nCONTRACTION_MAP = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n                                     flags=re.IGNORECASE|re.DOTALL)\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contraction_mapping.get(match)\\\n                                if contraction_mapping.get(match)\\\n                                else contraction_mapping.get(match.lower())                       \n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n    \n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4. Removing Special Characters**\n\nSpecial characters and symbols are usually non-alphanumeric characters or even occasionally numeric characters (depending on the problem), which add to the extra noise in unstructured text. Usually, simple regular expressions (regexes) can be used to remove them.","metadata":{}},{"cell_type":"code","source":"def remove_special_characters(text, remove_digits=False):\n    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n    text = re.sub(pattern, '', text)\n    return text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remove_special_characters(sample_text, remove_digits=False):","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **5. Lemmatization** \nLemmatization is very similar to stemming, where we remove word affixes to get to the base form of a word. However, the base form in this case is known as the root word, but not the root stem. The difference being that the root word is always a lexicographically correct word (present in the dictionary), but the root stem may not be so. Thus, root word, also known as the lemma, will always be present in the dictionary. Both nltk and spacy have excellent lemmatizers. We will be using spacy here.\n\n*Do note that the lemmatization process is considerably slower than stemming, because an additional step is involved where the root form or lemma is formed by removing the affix from the word if and only if the lemma is present in the dictionary.*","metadata":{}},{"cell_type":"code","source":"def lemmatize_text(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatize_text(sample_text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **6. Removing Stopwords**\nWords which have little or no significance, especially when constructing meaningful features from text, are known as stopwords or stop words. These are usually words that end up having the maximum frequency if you do a simple term or word frequency in a corpus. Typically, these can be articles, conjunctions, prepositions and so on. Some examples of stopwords are **a, an, the, and** the like.","metadata":{}},{"cell_type":"code","source":"def remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remove_stopwords(sample_text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **TextProcessing Pipelines**\n\nNow we put everything together","metadata":{}},{"cell_type":"code","source":"import math\ndef normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n                     accented_char_removal=True, text_lower_case=True, \n                     text_lemmatization=True, special_char_removal=True, \n                     stopword_removal=True, remove_digits=True):\n    num_docs = len(corpus)\n    processed_percent = 0\n    percent_increment = 5\n    processed_docs = 0\n    normalized_corpus = []\n    # normalize each document in the corpus\n    for doc in corpus:\n        # strip HTML\n        if html_stripping:\n            doc = strip_html_tags(doc)\n        # remove accented characters\n        if accented_char_removal:\n            doc = remove_accented_chars(doc)\n        # expand contractions    \n        if contraction_expansion:\n            doc = expand_contractions(doc)\n        # lowercase the text    \n        if text_lower_case:\n            doc = doc.lower()\n        # remove extra newlines\n        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n        # lemmatize text\n        if text_lemmatization:\n            doc = lemmatize_text(doc)\n        # remove special characters and\\or digits    \n        if special_char_removal:\n            # insert spaces between special characters to isolate them    \n            special_char_pattern = re.compile(r'([{.(-)!}])')\n            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n        # remove extra whitespace\n        doc = re.sub(' +', ' ', doc)\n        # remove stopwords\n        if stopword_removal:\n            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n        processed_docs += 1\n        normalized_corpus.append(doc)\n        \n        # Calculate and print the progress at each percentage increment\n        percent_done = math.floor((processed_docs / num_docs) * 100)\n        if percent_done >= processed_percent + percent_increment:\n            processed_percent = percent_done\n            print(f'Processed {processed_percent}% of documents')\n        \n    return normalized_corpus","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Uncommon this if you want content and title in one columns. this pre-process text and store the same\n#rawdf['full_text'] = rawdf[\"title\"].map(str)+ '. ' + rawdf[\"content\"]\n#rawdf['full_text']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop content and title column\n#news_text_df = rawdf.drop(['content', 'title'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pre-process content\ncleaned_content = normalize_corpus(rawdf['content'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pre-process title\ncleaned_title = normalize_corpus(rawdf['title'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store to new content and title columns\nrawdf['cleaned_content'] = cleaned_content\nrawdf = rawdf.assign(cleaned_content = cleaned_content)\n\nrawdf['cleaned_title'] = cleaned_title\nrawdf = rawdf.assign(cleaned_title = cleaned_title)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rawdf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Detecting Non-English Text**","metadata":{}},{"cell_type":"code","source":"!pip install langdetect","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langdetect import detect\n\ndef detect_language(text):\n    try:\n        detected_language = detect(text)\n        return detected_language\n    except Exception as e:\n        print(\"An error occurred:\", e)\n        return None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get certain row\nsample_text = rawdf.iloc[6].case_content","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detected_language = detect_language(sample_text)\nprint(f\"Detected language: {detected_language}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dropNAN(dataframe):\n    # Because the cleaned_content delete Russian text. We can safely detect Null and NaN values row\n    cleaned_dataframe = dataframe[dataframe.isna().any(axis=1)]\n        \n    # Drop NaN values\n    cleaned_dataframe =  dataframe.dropna() \n    cleaned_dataframe.reset_index(drop = True, inplace = True)\n    \n    return cleaned_dataframe  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dropNoise(df):\n    # Remove corupted data\n    # Filter rows based on the condition\n    noise_corpus = \"report typo following text simply click send typo report button complete report also include comment\"\n    noise_df = df['cleaned_content'].str.contains(noise_corpus)\n    \n    cleaned_df = df[~noise_df]\n    \n    return cleaned_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = dropNAN(rawdf)\ndf1 = dropNoise(rawdf)\ndf1.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n# Clean nonenglish text\ndef detect_nonenglish(corpus):\n    num_docs = len(corpus)\n    processed_percent = 0\n    percent_increment = 5\n    processed_docs = 0\n    detected_corpus = []\n    for doc in corpus:\n        # For other languages. We apply language detection to each content and create a new column 'language'\n        doc = detect_language(doc)\n        processed_docs += 1\n        detected_corpus.append(doc)\n        \n        # Calculate and print the progress at each percentage increment\n        percent_done = math.floor((processed_docs / num_docs) * 100)\n        if percent_done >= processed_percent + percent_increment:\n            processed_percent = percent_done\n            print(f'Processed {processed_percent}% of documents')\n    return detected_corpus","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detecting language for content column\nlang_col = detect_nonenglish(df1['content'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store new column\ndf1['language'] = lang_col","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter out non-English articles\ndf1 = df1[df1['language'] == 'en']\n\n# Drop the 'language' column as it's no longer needed\ndf1 = df1.drop(columns=['language'])\n\n# Save the cleaned DataFrame back to CSV\ndf1.to_csv('news_file_en_1.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Computing Vocabulary and Reduction Rate**\nThis is the result of the below code\n\n**Report of news file 1**\n* Size of vocabulary in news content:  2063986\n* Size of vocabulary in cleaned news content:  558106\n* Reduction  rate of vocabulary size: 72.96%\n\n**Report of news file 2**\n* Size of vocabulary in news content:  2048903\n* Size of vocabulary in cleaned news content:  527854\n* Reduction  rate of vocabulary size: 74.24%\n\n**Report of news file 3**\n* Size of vocabulary in news content:  2021347\n* Size of vocabulary in cleaned news content:  539056\n* Reduction  rate of vocabulary size: 73.33%\n\n**Report of news file 4**\n* Size of vocabulary in news content:  1521905\n* Size of vocabulary in cleaned news content:  478514\n* Reduction  rate of vocabulary size: 68.56%\n\n**Report of news file 5**\n* Size of vocabulary in news content:  1646381\n* Size of vocabulary in cleaned news content:  474122\n* Reduction  rate of vocabulary size: 71.20%\n\n**Report of news file 6**\n* Size of vocabulary in news content:  2041941\n* Size of vocabulary in cleaned news content:  552321\n* Reduction  rate of vocabulary size: 72.95%\n\n**Report of news file 7**\n* Size of vocabulary in news content:  1768009\n* Size of vocabulary in cleaned news content:  455136\n* Reduction  rate of vocabulary size: 74.26%","metadata":{}},{"cell_type":"code","source":"# Compute the size of the vocabulary after removing stopwords and lemitization\n# Compute the reduction rate of the vocabulary size after removing stopwords & lemitization.\ndef vocab_reduct(dataframe):\n    # Tokenization content and cleaned_content\n    content_tokens = [word for line in dataframe['content'] for word in str(line).split()]\n    cleaned_content_tokens = [word for line in dataframe['cleaned_content'] for word in str(line).split()]\n    \n    #Compute vocabulary sizes\n    content_vocab_size = len(set(content_tokens))\n    cleaned_content_vocab_size = len(set(cleaned_content_tokens))\n    \n    #Compute reduction rate\n    reduction_rate = ((content_vocab_size - cleaned_content_vocab_size) / content_vocab_size) * 100\n    \n    print(\"Size of vocabulary in news content: \", content_vocab_size)\n    print(\"Size of vocabulary in cleaned news content: \", cleaned_content_vocab_size)\n    print(\"Reduction  rate of vocabulary size: {:.2f}%\".format(reduction_rate))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Incrementally computing each csv file\n# Process each CSV file\npath_name = f'/kaggle/input/fake-news-content-and-title'\nfor i in range(1, 8):\n    file_path = f'{path_name}/news_file_en_{i}.csv'\n    # Read the dataframe\n    chunk_df = pd.read_csv(file_path)\n    print(f'Report of news file {i}')\n    vocab_reduct(chunk_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}