{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8135831,"sourceType":"datasetVersion","datasetId":4809434}],"dockerImageVersionId":30702,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"First, We will import necessary Python packages and modules required for various functionalities such as data handling, model building, evaluation metrics, and preprocessing.BERT fit for this because it understands language well and can accurately classify fake news by using pre-learned representations.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the dataset from a CSV file and extract the relevant features ('cleaned_content' for input text and 'type' for target labels.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/fake-binary-reclass/reclassified_1.csv')\nX = data['cleaned_content']\ny = data['type']  \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert categorical target labels ('type') into numerical labels using LabelEncoder.","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"split the dataset into training and testing sets using a 80-20 split ratio.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use the pre-trained BERT tokenizer to tokenize and encode the input sequences into numerical format suitable for BERT model input.","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\nmax_len = 256  \ntrain_encodings = tokenizer(list(X_train), truncation=True, padding='max_length', max_length=max_len, return_tensors='pt')\ntest_encodings = tokenizer(list(X_test), truncation=True, padding='max_length', max_length=max_len, return_tensors='pt')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert the numerical labels into PyTorch tensors.And create PyTorch datasets using the tokenized input sequences, attention masks, and labels.","metadata":{}},{"cell_type":"code","source":"train_labels = torch.tensor(y_train)\ntest_labels = torch.tensor(y_test)\ntrain_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\ntest_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create data loaders for training and testing datasets to facilitate batch-wise processing during training and evaluation. We made batch size of 40, because model was taking so much time to train.","metadata":{}},{"cell_type":"code","source":"batch_size = 40\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=RandomSampler(train_dataset))\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=SequentialSampler(test_dataset))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the pre-trained BERT model for sequence classification. The number of labels is set to the number of unique classes in the target variable. And freeze the parameters of the base BERT model to prevent them from being updated during training.","metadata":{}},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n\nfor param in model.base_model.parameters():\n    param.requires_grad = False\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the optimizer to only optimize the parameters of the final classification layer. And train the model using the training data and evaluate its performance on the testing data. ","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(model.classifier.parameters(), lr=2e-5, eps=1e-8)\n\n# Define number of epochs\nepochs = 3\n\n# Total number of training steps\ntotal_steps = len(train_loader) * epochs\n\n# Create learning rate scheduler\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n# Define training loop\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Iterate over epochs, batches, calculating loss, and updating model parameters during training, and making predictions and calculating evaluation metrics during testing.","metadata":{}},{"cell_type":"code","source":"for epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n        \n        model.zero_grad()\n        \n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        \n        loss = outputs.loss\n        total_loss += loss.item()\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        scheduler.step()\n    \n    avg_train_loss = total_loss / len(train_loader)\n    print(f'Epoch {epoch + 1}/{epochs}')\n    print(f'Training Loss: {avg_train_loss}')\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluate the model.","metadata":{}},{"cell_type":"code","source":"model.eval()\npredictions = []\ntrue_labels = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].numpy()\n        \n        outputs = model(input_ids, attention_mask=attention_mask)\n        \n        logits = outputs.logits\n        preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n        \n        predictions.extend(preds)\n        true_labels.extend(labels)\n\npredictions = np.array(predictions)\ntrue_labels = np.array(true_labels)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculate the evalutation metrics","metadata":{}},{"cell_type":"code","source":"accuracy = accuracy_score(true_labels, predictions)\nprecision = precision_score(true_labels, predictions)\nrecall = recall_score(true_labels, predictions)\nf1 = f1_score(true_labels, predictions)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Evaluation Metrics for Advanced Model:\")\nprint(\"Accuracy:\", accuracy)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\nprint(\"Precision:\", precision)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T23:28:07.480053Z","iopub.execute_input":"2024-04-20T23:28:07.480871Z","iopub.status.idle":"2024-04-20T23:28:07.530518Z","shell.execute_reply.started":"2024-04-20T23:28:07.480808Z","shell.execute_reply":"2024-04-20T23:28:07.529212Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Evaluation Metrics for Advanced Model:\nAccuracy: 0.69\nRecall: 0.89\nF1 Score: 0.77\nPrecision: 0.69\n","output_type":"stream"}]}]}